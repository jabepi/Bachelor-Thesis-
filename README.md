# Bachelor-Thesis-
Analyzed performance of multithreaded programs on Non-Uniform Memory Access (NUMA) architectures, studying the impact of parallel accesses to shared memory regions.

# Abstract 
NUMA systems represent a highly scalable and highly interesting alternative in the field of shared memory systems. Understanding their behavior in terms of data locality and thread affinity to these data is crucial for efficient execution.

For this reason, in this work, we conduct a detailed examination of the behavior of NUMA shared memory multithreaded systems, focusing our study on the impact of placing two threads on nodes in relation to the data they access, in the case of programs where memory access intensity predominates over computational intensity. On one hand, we study locality by analyzing the performance of a single thread in execution, performing local and remote memory operations. Subsequently, we introduce a second thread, observing possible effects due to resource sharing. Finally, we analyze affinity by studying the effects produced by access to a vector of data shared by two threads in parallel.

The tests have been carried out on a NUMA system with homogeneous interconnections, with 4 nodes interconnected in a ring. The results show that, firstly, local data access offers performance up to 3 times higher than remote accesses. Secondly, resource sharing, particularly of memory buses and the node interconnection bus, can significantly penalize performance when parallel programs are data-access intensive. Thirdly, performance in accesses to shared data can be influenced by coherence protocols, tripling the execution time of threads that access their local data when synchronizing with the thread that performs the access from a remote node. Finally, the effects of this synchronization can be mitigated by introducing a delay in the execution of one of the threads, limiting the effects of memory access conflicts and bus contention. This allows the leading thread to approach the performance of an isolated execution and the delayed thread to take advantage of the first's data preloading, improving its performance despite the introduced delay.

In this way, this study identifies situations of performance loss, thereby enabling the application of possible corrections through access scheduling techniques, thread and/or data migration, etc.
